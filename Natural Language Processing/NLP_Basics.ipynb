{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data collection/ Corpus\n",
    "Here, we will be using our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ''' Ram loves to play cricket. Sita is the wife of Ram. Ram and Laxman are brothers. Bharat likes archery.'''\n",
    "sentence = \" Good Evening, Saurabh how are you doing today? \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization and Stop words removal\n",
    "#### (a) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SRISHTI\n",
      "[nltk_data]     GUPTA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download(\"punkt\") # punkt is an inbuilt nltk corpus which contains english words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[' Ram loves to play cricket.', 'Sita is the wife of Ram.', 'Ram and Laxman are brothers.', 'Bharat likes archery.']\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize() function tokenizes the document/data/corpus into list of sentences.\n",
    "sents = sent_tokenize(data)\n",
    "print(len(sents))\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['Good', 'Evening', ',', 'Saurabh', 'how', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize() function tokenizes sentences into list of words.\n",
    "words = word_tokenize(sentence)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SRISHTI\n",
      "[nltk_data]     GUPTA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 'a', 'me', 'themselves', 'be', 'shouldn', 'wouldn', 'through', 'both', 'few', 'herself', 'other', \"needn't\", 'doesn', 'yourself', 'very', 'some', 'itself', \"you've\", 'but', 'himself', 'ours', 'mightn', \"mightn't\", 'haven', 'ourselves', 'by', \"won't\", 'same', 'won', 'with', 'hasn', 'all', 'no', 'ma', 'your', 'further', 'the', 'then', 'm', 'isn', 'before', \"wouldn't\", \"you're\", 'his', 'which', 'until', 'is', 'off', 'more', \"should've\", 'once', 'into', 'out', 'their', 'to', 'had', 'any', 'if', 'so', 't', 'yours', 'its', 'who', 'doing', 'at', 'ain', 'shan', 'hadn', 'he', 'below', 'can', 'above', 'too', 'over', 'needn', \"hadn't\", 'my', 'only', 'now', \"didn't\", 'than', 'how', 'on', 'don', \"don't\", 'y', 'down', 'under', 'i', 'wasn', 'not', 'd', 'up', 'and', 'here', \"she's\", 'did', 'when', 'she', 'whom', 'myself', 'such', 's', 'was', 'am', 'while', 'after', 'they', 'own', 'couldn', 'between', 'what', \"shan't\", 'against', 'mustn', \"aren't\", \"shouldn't\", 'as', 'were', 'should', 'do', 'our', 'having', 'being', 'or', \"mustn't\", 'those', 'hers', \"doesn't\", 'you', 'it', 'during', \"couldn't\", 'them', 'nor', \"you'd\", 'weren', 'just', 'this', \"wasn't\", 'where', 'each', 'in', 'll', 'him', \"it's\", 've', 'didn', 'most', 'we', 'an', 'because', \"weren't\", 'again', 'from', 'of', 'o', \"isn't\", 'that', 'aren', 'about', 'have', 'theirs', 'there', 'does', 'why', \"hasn't\", 'her', 'will', 're', 'has', \"that'll\", 'for', 'yourselves', 'are', \"haven't\", \"you'll\", 'been'}\n"
     ]
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "# sw is a set of stopwords generally used in english language and we can ignore them while making vocab.\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words,stopwards):\n",
    "    useful_words = [i for i in words if i  not in stopwards]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'Evening', ',', 'Saurabh', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "useful_words = remove_stopwords(words,sw) # 'how','are', 'you' are removed from the list of words as they were in sw.\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - One thing to be kept in mind is that, we always do not need to use these inbuilt stopwords. We can create list of stopwords according to our own needs and usecase. Otherwise, sometimes sentiments can get changed using inbuilt functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "# We can take help from online resource such as regexpal.com to play with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = \"My email id is abc@xyz.in\"\n",
    "sent_2 = \"Ram has 3 cars, 2 tractors and 8 bikes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'email', 'id', 'is', 'abc', '@', 'xyz.in']\n"
     ]
    }
   ],
   "source": [
    "words_1 = word_tokenize(sent_1)\n",
    "print(words_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We can notice in above example, the email-id got tokenized into several parts by default and in certain cases we do not need this to happen. So, we can use regex to customise our tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'email', 'id', 'is', 'abc@xyz.in']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_1 = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_words_1 = tokenizer_1.tokenize(sent_1)\n",
    "print(useful_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'has', '3', 'cars', ',', '2', 'tractors', 'and', '8', 'bikes']\n"
     ]
    }
   ],
   "source": [
    "words_2 = word_tokenize(sent_2)\n",
    "print(words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Suppose in above example we do not need numbers then we can use regex to discard numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', 'has', 'cars', 'tractors', 'and', 'bikes']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_2 = RegexpTokenizer('[a-zA-Z]+')\n",
    "useful_words_2 = tokenizer_2.tokenize(sent_2)\n",
    "print(useful_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stemming and Lemmatiztion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Stemming\n",
    " - NLTK provides us three different stemming options which are-\n",
    " 1. Snowball Stemmer\n",
    " 2. Porter Stemmer\n",
    " 3. Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' Ram is a singer and loves singing. Ram has been practicing singing since he was 6 years.\n",
    "He sings beutifully and everyone gets mesmerized when he sings.''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"singing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sing'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"sings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem(\"jumps\"))\n",
    "print(ps.stem(\"jumping\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with SnowballStemmer\n",
    "\n",
    " - SnowballStemmer is a multi-lingual stemmer and it supports more than one language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('loving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\SRISHTI\n",
      "[nltk_data]     GUPTA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dancing'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dance'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmatize('dances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Building a Vocab and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy corpus contains 4 documents from Sports, Politics, Movie, Technology catogory. Each document can have 1 or more sentences\n",
    "dummy_corpus = [ 'Virat Kohli is classy cricketer and he dominates in all three formats of the game.',\n",
    "'Narendra Modi is the prime minister of India and comes from Bhartiya Janta Party.',\n",
    "'Dangal is a movie based on two sisters Geeta and Babita who are wrestlers.',\n",
    "'Iphone 12 got launched in India recently by Apple. ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fit_transform(), fit learns what the dictionary is from the given document and transform converts the data into vectorized format\n",
    "vectorized_corpus = cv.fit_transform(dummy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0])) # It tells there are 43 unique words in the entire corpus.\n",
    "vectorized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'virat': 40, 'kohli': 25, 'is': 23, 'classy': 9, 'cricketer': 11, 'and': 2, 'he': 19, 'dominates': 13, 'in': 20, 'all': 1, 'three': 38, 'formats': 14, 'of': 31, 'the': 37, 'game': 16, 'narendra': 30, 'modi': 28, 'prime': 34, 'minister': 27, 'india': 21, 'comes': 10, 'from': 15, 'bhartiya': 7, 'janta': 24, 'party': 33, 'dangal': 12, 'movie': 29, 'based': 6, 'on': 32, 'two': 39, 'sisters': 36, 'geeta': 17, 'babita': 5, 'who': 41, 'are': 4, 'wrestlers': 42, 'iphone': 22, '12': 0, 'got': 18, 'launched': 26, 'recently': 35, 'by': 8, 'apple': 3}\n"
     ]
    }
   ],
   "source": [
    "# To get the idea how mapping is done\n",
    "print(cv.vocabulary_)\n",
    "# It is a dictionary (key-value) pair which shows at what location words are present in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(cv.vocabulary_.keys())) # This shows there are 43 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = vectorized_corpus[0]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['all', 'and', 'classy', 'cricketer', 'dominates', 'formats',\n",
       "        'game', 'he', 'in', 'is', 'kohli', 'of', 'the', 'three', 'virat'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, it is a bag of words, we cannot get the same order but it still gives an idea of all the unique word in that document.\n",
    "cv.inverse_transform(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization with stop words removal\n",
    "\n",
    " - When we create a  countVectorizer() object, in this we can pass our own custom tokenizer which can be used to remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(sent):\n",
    "    words = tokenizer.tokenize(sent.lower())\n",
    "    words = remove_stopwords(words,sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentance', 'written', 'testing.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(\"This is a sentance written for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer= myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(dummy_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[[0 0 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))\n",
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['classy', 'cricketer', 'dominates', 'formats', 'game.', 'kohli',\n",
       "        'three', 'virat'], dtype='<U10'),\n",
       " array(['bhartiya', 'comes', 'india', 'janta', 'minister', 'modi',\n",
       "        'narendra', 'party.', 'prime'], dtype='<U10'),\n",
       " array(['babita', 'based', 'dangal', 'geeta', 'movie', 'sisters', 'two',\n",
       "        'wrestlers.'], dtype='<U10'),\n",
       " array(['apple.', 'got', 'india', 'iphone', 'launched', 'recently'],\n",
       "       dtype='<U10')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - We dont need to learn new corpus for test data, we use our trained corpus\n",
    "  - For test data-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [' This is a test corpus.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - fit_transform() methid is used for training data and transform() method is used for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
